# https://github.com/openmarmot/aws-cft-llama-cpp

# Refs
#  llama.cpp repo  : https://github.com/ggerganov/llama.cpp
#  hugging face : https://huggingface.co/

# I have a model hosted by Hugging Face as the default here. 
# Please be mindful of the bandwidth you are using as they generously host for free and help this community prosper.
# If you find yourself downloading something repeatedly please consider storing it locally in a s3 bucket

# !! Important Notes !!
# - This is designed to be a minimal template for hobbyists that focuses on keeping costs low
# - If your budget allows consider an autoscaling cluster of ec2 instances on a private subnet with a ALB and WAF in front
# - Be mindful of excessively downloading models. If you are launching a lot of instance consider creating an image with the model setup
#   instead of downloading it each time.
# - For faster model performance consider a GPU instance type. Note that you will need to install some Nvidia libraries and modify the Make settings
#   I might produce a guide on this in the future
# - The template as designed uses port 80 - this is unecrypted. Any data you submit will go over the internet unecrypted. If you want HTTPS the 
#   best way to get it would be to put the instance in a private subnet and put a load balancer in front of it
# - the setup launches a web server. Go to the public ip of the serve on port 80 to access it. http://<server ip>
# - if you need to work on the server it is all setup to aws aws ssm connect

# to restart the web server after power cycling the instance, do the following 
# - connect to the instance with ssm (connect button)
# sudo su 
# cd /opt/llama.cpp
# ./server -m models/<model name here> -c 2048 --host 0.0.0.0 --port 80

# !! This template as currently designed will cost around $120/month if you leave the instace on all month as of Amazon 2023 pricing.
# Use at your own risk!
 


AWSTemplateFormatVersion: '2010-09-09'
Description: AWS CloudFormation Template that creates and runs a Llama.cpp server
Parameters:
  NamePrefix:
    Description: A prefix to use when naming objects created by this template
    Type: String
    Default: 'cft-llama'
  InstanceType:
    Description: EC2 Instance Type - about 8 GB of ram for a 7b model, about 16 for a 13b. The more CPU the better
    Type: String
    Default: c5a.xlarge
  AmiId:
    Description: Amazon Linux 2023 AMI ID
    Type: 'AWS::SSM::Parameter::Value<AWS::EC2::Image::Id>'
    Default: '/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-x86_64'
  VolumeSize:
    Description: Size of the EC2 instance volume in GB
    Type: Number
    Default: 25
  VPC:
    Description: The VPC for the AWS objects
    Type: 'AWS::EC2::VPC::Id'
  SubnetId:
    Description: VPC PUBLIC subnet ID
    Type: 'AWS::EC2::Subnet::Id'
  NetAllowedIn:
    Description: 'CIDR Network Allowed to access the server (X.X.X.X/XX). This could be your public ip with a /32'
    Type: String
  ModelURL:
    Description: 'URL for the model you want to download. The default model is aobut 4 GB. Be mindful about not abusing by excessive downloading'
    Type: String
    Default: 'https://huggingface.co/TheBloke/vicuna-7B-v1.5-GGML/resolve/main/vicuna-7b-v1.5.ggmlv3.q4_K_M.bin'
  ModelFileName:
    Description: 'The file name you want to save the model as (x.bin)'
    Type: String
    Default: 'vicuna-7b-v1.5.ggmlv3.q4_K_M.bin'
  
Resources:

  IAMRole:
    Type: 'AWS::IAM::Role'
    Properties:
      RoleName: !Sub '${NamePrefix}-ec2-role'
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: 'Allow'
            Principal:
              Service:
                - 'ec2.amazonaws.com'
            Action:
              - 'sts:AssumeRole'
      Policies:
        - PolicyName: ssm-connect-minimal-permissions
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: 'Allow'
                Action:
                  - 'ssm:UpdateInstanceInformation'
                  - 'ssmmessages:CreateControlChannel'
                  - 'ssmmessages:CreateDataChannel'
                  - 'ssmmessages:OpenControlChannel'
                  - 'ssmmessages:OpenDataChannel'
                Resource: '*'

  InstanceProfile:
    Type: 'AWS::IAM::InstanceProfile'
    Properties:
      InstanceProfileName: !Sub '${NamePrefix}-instance-profile'
      Roles:
        - Ref: IAMRole

  InstanceSecurityGroup:
    Type: 'AWS::EC2::SecurityGroup'
    Properties:
      GroupDescription: !Sub '${NamePrefix} security group'
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 80
          ToPort: 80
          CidrIp: !Ref NetAllowedIn
          Description: 'Allow http inbound to the webserver'
      SecurityGroupEgress:
        - IpProtocol: -1
          CidrIp: 0.0.0.0/0
          Description: 'Allow Outbound'
      VpcId: !Ref VPC

  LlamaCppServer:
    Type: 'AWS::EC2::Instance'
    Properties:
      InstanceType: !Ref InstanceType
      ImageId: !Ref AmiId
      IamInstanceProfile: !Ref InstanceProfile
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: !Ref VolumeSize
            Encrypted: true
      SecurityGroupIds:
        - !GetAtt InstanceSecurityGroup.GroupId
      SubnetId: !Ref SubnetId
      UserData:
        Fn::Base64: !Sub |
          #!/usr/bin/env bash
          yum install make automake gcc gcc-c++ kernel-devel python3-virtualenv screen git -y
          cd /opt
          git clone https://github.com/ggerganov/llama.cpp.git
          cd llama.cpp
          # compile llama.cpp
          make
          # download the model file 
          wget -P models ${ModelURL}
          # start server
          nohup ./server -m models/${ModelFileName} -c 2048 --host 0.0.0.0 --port 80 &
          # Signal result to CloudFormation
          /opt/aws/bin/cfn-signal -e $? --stack "${AWS::StackName}" --resource "LlamaCppServer" --region "${AWS::Region}"
      Tags:
        - Key: Name
          Value: !Sub '${NamePrefix} llama.cpp server'
    CreationPolicy:
      ResourceSignal:
        Timeout: PT10M

Outputs:
  InstancePrivateIp:
    Description: Access the Web Server Here
    Value: !Join ['',['http://',!GetAtt LlamaCppServer.PublicIp]]
